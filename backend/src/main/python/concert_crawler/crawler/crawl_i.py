import requests
from bs4 import BeautifulSoup
import re
from config import I_BASE_URL, HEADERS, FILTERED_VENUES

class ConcertCrawler:
    def __init__(self):
        self.seen_ids = set()
        self.duplicate_pages = 0
        self.MAX_DUPLICATE_PAGES = 3
    
    def get_concerts_from_page(self, page):
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê³µì—° ì •ë³´ ëª©ë¡ ì¶”ì¶œ"""
        params = {
            "sReqMainCategory": "000003",  # ì½˜ì„œíŠ¸ ì¹´í…Œê³ ë¦¬
            "Page": page
        }
        
        res = requests.get(I_BASE_URL, headers=HEADERS, params=params)
        soup = BeautifulSoup(res.text, "html.parser")
        show_blocks = soup.select("td[width='375']")
        
        if not show_blocks:
            return [], False
        
        new_data_found = False
        concerts = []
        
        print(f"\nğŸ“„ {page}í˜ì´ì§€ ({len(show_blocks)}ê°œ ê³µì—°)\n")
        
        for block in show_blocks:
            a_tag = block.find("a", href="#")
            title = a_tag.text.strip() if a_tag else "ì œëª© ì—†ìŒ"
            onclick = a_tag.get("onclick", "")
            match = re.search(r"goDetail\('(\d+)'\)", onclick)
            show_id = match.group(1) if match else None
            
            # ì¤‘ë³µ ê³µì—°ì€ ê±´ë„ˆë›°ê¸°
            if show_id is None or show_id in self.seen_ids:
                continue
            
            self.seen_ids.add(show_id)
            new_data_found = True
            
            text = block.get_text(separator=" ", strip=True)
            date_match = re.search(r"ì¼ì‹œ\s*:\s*([0-9.\s~]+)", text)
            place_tag = block.find("a", href=re.compile("PlacedbInfo.asp"))
            artist_tags = block.find_all("a", href=re.compile("artistdb/detail.asp"))
            artists = [a.text.strip() for a in artist_tags]
            artist_text = ", ".join(artists) if artists else "ì¶œì—°ì ì •ë³´ ì—†ìŒ"
            poster_td = block.find_previous_sibling("td", attrs={"width": "90"})
            poster_img = poster_td.find("img")["src"] if poster_td and poster_td.find("img") else "í¬ìŠ¤í„° ì—†ìŒ"
            
            date = date_match.group(1).strip() if date_match else "ë‚ ì§œ ì—†ìŒ"
            place = place_tag.text.strip() if place_tag else "ì¥ì†Œ ì—†ìŒ"
            
            detail_url = f"http://www.playdb.co.kr/playdb/playdbdetail.asp?sReqPlayNo={show_id}"
            
            # íŠ¹ì • ê³µì—°ì¥ í•„í„°ë§
            if any(keyword in place for keyword in FILTERED_VENUES):
                concert_data = {
                    'title': title,
                    'place': place,
                    'date': date,
                    'artist': artists[0] if artists else None,
                    'poster_url': poster_img if poster_img != "í¬ìŠ¤í„° ì—†ìŒ" else None,
                    'detail_url': detail_url,
                    'show_id': show_id
                }
                
                concerts.append(concert_data)
                
                print(f"ğŸŸï¸ ê³µì—°ëª…: {title}")
                print(f"ğŸ“ ì¥ì†Œ: {place}")
                print(f"ğŸ—“ï¸ ì¼ì‹œ: {date}")
                print(f"ğŸ‘¤ ì¶œì—°: {artist_text}")
                print(f"ğŸ”— ìƒì„¸í˜ì´ì§€: {detail_url}")
                print("-" * 50)
        
        return concerts, new_data_found
    
    def crawl_concerts(self):
        """í˜ì´ì§€ë³„ë¡œ ê³µì—° ì •ë³´ ìˆ˜ì§‘"""
        page = 1
        all_concerts = []
        
        while True:
            concerts, new_data_found = self.get_concerts_from_page(page)
            
            if not concerts:
                print(f"\nğŸ›‘ ê³µì—°ì´ ë” ì´ìƒ ì—†ìŠµë‹ˆë‹¤. (í˜ì´ì§€ {page})")
                break
            
            all_concerts.extend(concerts)
            
            # ì¤‘ë³µëœ ê³µì—°ë§Œ ìˆë˜ í˜ì´ì§€ê°€ ê³„ì† ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
            if not new_data_found:
                self.duplicate_pages += 1
                if self.duplicate_pages >= self.MAX_DUPLICATE_PAGES:
                    print("\nğŸ›‘ ë” ì´ìƒ ìƒˆë¡œìš´ ê³µì—°ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
                    break
            else:
                self.duplicate_pages = 0  # ìƒˆ ë°ì´í„°ê°€ ìˆì—ˆìœ¼ë©´ ì´ˆê¸°í™”
            
            page += 1
        
        print(f"\nì „ì²´ {len(all_concerts)}ê°œì˜ ê³µì—° ì •ë³´ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        return all_concerts