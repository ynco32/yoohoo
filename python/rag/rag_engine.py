import os
import logging
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

def split_documents(paragraphs, concert_info):
    """ë¬¸ë‹¨ë“¤ì„ ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    logger.info("ë¬¸ë‹¨ì„ ì²­í¬ë¡œ ë¶„í•  ì¤‘...")
    
    # RecursiveCharacterTextSplitter ì„¤ì •
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,  # ì²­í¬ í¬ê¸°
        chunk_overlap=50,  # ì²­í¬ ê°„ ì¤‘ë³µ (ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´)
        separators=["\n\n", "\n", ".", ",", " ", ""]
    )
    
    all_texts = []
    all_metadatas = []
    
    # ê° ë¬¸ë‹¨ì„ ì²˜ë¦¬
    for i, paragraph in enumerate(paragraphs):
        text = paragraph["text"]

        para_metadata = paragraph["metadata"]  # ê·¸ë£¹í™”ëœ ë¬¸ë‹¨ì—ëŠ” í•­ìƒ metadataê°€ ìˆìŒ

        metadata = {
            "concert_id": concert_info["concert_id"],
            "paragraph_id": i,
            "top_y": para_metadata.get("top_y"),
            "bottom_y": para_metadata.get("bottom_y"),
            "category": para_metadata.get("category", "ì¼ë°˜ ì •ë³´")
        }

        
        # ì½˜ì„œíŠ¸ ì •ë³´ ë©”íƒ€ë°ì´í„° ì¶”ê°€ 
        metadata["concert_name"] = concert_info["concert_name"]
        metadata["arena_name"] = concert_info["arena_name"]
        
        # ì•„í‹°ìŠ¤íŠ¸ ì •ë³´ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€ (ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ) 
        artists = concert_info.get("artist_name", [])
        if artists:
            metadata["artists"] = ", ".join(artists)
        
        # í‹°ì¼“íŒ… ì •ë³´ ì¶”ê°€ 
        metadata["ticketing_platform"] = concert_info.get("ticketing_platform")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        splits = text_splitter.split_text(text)
        
        # ê° ë¶„í• ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for j, split in enumerate(splits):
            all_texts.append(split)
            split_metadata = metadata.copy()
            split_metadata["chunk_id"] = f"{i}-{j}"
            split_metadata["text_snippet"] = split[:100]  # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë¯¸ë¦¬ë³´ê¸°
            all_metadatas.append(split_metadata)
    
    logger.info(f"ì´ {len(all_texts)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return all_texts, all_metadatas


def expand_query(original_query, conversation_history=None):
    """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ í™•ì¥í•˜ì—¬ ë” ê´€ë ¨ì„± ë†’ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤."""
    logger.info(f"ì›ë³¸ ì¿¼ë¦¬: '{original_query}'")
    
    # ëŒ€í™” ë§¥ë½ì´ ìˆëŠ” ê²½ìš° ì´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    context_text = ""
    if conversation_history and len(conversation_history) > 0:
        # ìµœê·¼ 1-2ê°œ ëŒ€í™”ë§Œ í¬í•¨ (ë§¥ë½ ì°½ ì œí•œ)
        recent_history = conversation_history[-2:] if len(conversation_history) > 2 else conversation_history
        context_lines = []
        for turn in recent_history:
            context_lines.append(f"ì‚¬ìš©ì: {turn['user']}")
            context_lines.append(f"ë´‡: {turn['bot']}")
        context_text = "\n".join(context_lines)
        logger.info(f"ìµœê·¼ {len(recent_history)}ê°œ ëŒ€í™” ë§¥ë½ í¬í•¨")
    
    # ë§¥ë½ í¬í•¨ ì—¬ë¶€ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    if context_text:
        context_part = f"ìµœê·¼ ëŒ€í™” ë§¥ë½:\n{context_text}\n"
    else:
        context_part = ""
    
    # í”„ë¡¬í”„íŠ¸ ê°œì„  (ëª…í™•í•œ ì§€ì¹¨ê³¼ ì˜ˆì‹œ ì¶”ê°€) ğŸ‘ˆ
    expand_prompt = f"""
    ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¥ ë„êµ¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ê²€ìƒ‰ì— ìœ ìš©í•œ ì¶”ê°€ í‚¤ì›Œë“œë¥¼ ì œê³µí•˜ì„¸ìš”.
    
    ì›ë³¸ ì§ˆë¬¸: "{original_query}"
    
    {context_part}
    
    ì§€ì‹œì‚¬í•­:
    1. ì£¼ì–´ì§„ ì§ˆë¬¸ì„ í™•ì¥í•˜ì—¬ ë” ë§ì€ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ í‚¤ì›Œë“œë¥¼ ì¶”ê°€í•˜ì„¸ìš”
    2. ì›ë³¸ ì§ˆë¬¸ì˜ í•µì‹¬ ì£¼ì œë¥¼ íŒŒì•…í•˜ì„¸ìš”
    3. í•µì‹¬ ì£¼ì œì™€ ì§ì ‘ ê´€ë ¨ëœ ë™ì˜ì–´ì™€ ìœ ì‚¬ì–´ë§Œ 2~5ê°œ ì¶”ê°€í•˜ì„¸ìš”
    4. ì£¼ì œì™€ ê´€ë ¨ ì—†ëŠ” í‚¤ì›Œë“œëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
    5. ì›ë³¸ ì§ˆë¬¸ì´ ìš°ì„ ì‹œ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
    6. ì£¼ìš” ë‹¨ì–´ì˜ ë‹¤ë¥¸ í‘œí˜„ë§Œ ì¶”ê°€í•˜ê³ , ë‹¤ë¥¸ ì£¼ì œë¡œ í™•ì¥í•˜ì§€ ë§ˆì„¸ìš”
    
    ì˜ˆì‹œ:
    ì›ë³¸: "ì¢Œì„ ìˆ˜ëŠ” ëª‡ ê°œì¸ê°€ìš”?"
    ì¢‹ì€ í™•ì¥: ì¢Œì„ ìˆ˜ëŠ” ëª‡ ê°œ ì¢Œì„ ì¢Œì„ë°°ì¹˜ë„ ì¢Œì„ìˆ˜
    ë‚˜ìœ í™•ì¥: ì¢Œì„ ìˆ˜ ëª‡ ê°œ ë§¤í‘œì†Œ ìœ„ì¹˜ í‹°ì¼“ë¶€ìŠ¤ ìœ„ì¹˜ í‹°ì¼“ ìˆ˜ë ¹ ë°©ë²•

    ì›ë³¸: "ë§¤í‘œì†Œê°€ ì–´ë””ì¸ê°€ìš”?"
    ì¢‹ì€ í™•ì¥: ë§¤í‘œì†Œê°€ ì–´ë”” ë§¤í‘œì†Œìœ„ì¹˜ ìœ„ì¹˜ ë§¤í‘œì¥ì†Œ
    ë‚˜ìœ í™•ì¥: ë§¤í‘œì†Œê°€ ì–´ë”” ì˜ˆë§¤ ë°©ë²• ì˜ˆë§¤ ì·¨ì†Œ ìˆ˜ìˆ˜ë£Œ

    ì¶œë ¥ ì˜ˆì‹œ:
    ì„ ì˜ˆë§¤ ì¼ì • í‹°ì¼“ ì˜ˆë§¤ ì‹œì‘ ë‚ ì§œ ì½˜ì„œíŠ¸ í‹°ì¼“íŒ… ë‚ ì§œ íŒ¬í´ëŸ½ ì‚¬ì „ì˜ˆì•½
    
    ì¶œë ¥:
    """
    
    # OpenAI API í˜¸ì¶œ
    from openai import OpenAI
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ê°œì„  ğŸ‘ˆ
            {"role": "system", "content": "ë‹¹ì‹ ì€ ê²€ìƒ‰ í‚¤ì›Œë“œ í™•ì¥ ë„êµ¬ì…ë‹ˆë‹¤. ì§€ì‹œì‚¬í•­ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”."},
            {"role": "user", "content": expand_prompt}
        ],
        temperature=0.3,
    )
    
    expanded_query = response.choices[0].message.content.strip()
    
    # ë¶ˆí•„ìš”í•œ ì ‘ë‘ì‚¬ì™€ ë”°ì˜´í‘œ ì œê±° (ì‹ ê·œ ì¶”ê°€) ğŸ‘ˆ
    unwanted_prefixes = [
        "ì¶œë ¥:", "í™•ì¥ëœ ê²€ìƒ‰ì–´:", "í‚¤ì›Œë“œ:", "ê²€ìƒ‰ì–´:", "ì¿¼ë¦¬:", 
        "\"", "\'", """, """
    ]
    
    for prefix in unwanted_prefixes:
        if expanded_query.startswith(prefix):
            expanded_query = expanded_query[len(prefix):].strip()
    
    # ë¶ˆí•„ìš”í•œ ë”°ì˜´í‘œ ì œê±°
    expanded_query = expanded_query.strip('"\'')

    final_query = f"{original_query} {expanded_query} {original_query}"
    
    logger.info(f"ì¿¼ë¦¬ í™•ì¥: '{original_query}' â†’ '{final_query}'")
    
    return expanded_query


# gpt ë‹¤ì‹œ ì‹œë„ ver
def create_rag_chain(vectorstore):
    """RAG ì§ˆì˜ì‘ë‹µ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    logger.info("RAG ì²´ì¸ ìƒì„± ì¤‘...")
    
    # ë‹¨ìˆœ ë˜í¼ ê°ì²´ ìƒì„± (ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ê¸°ë§Œ í¬í•¨)
    class SimpleRAGChain:
        def __init__(self, retriever, vectorstore):
            self.retriever = retriever
            self.vectorstore = vectorstore
            self.conversation_history = []

            
        def __call__(self, query_dict):
            # ë”ë¯¸ ë©”ì„œë“œ (ì‹¤ì œë¡œëŠ” query_rag_systemì—ì„œ ì²˜ë¦¬)
            return {"result": "This is a placeholder", "source_documents": []}
    
    # ê¸°ë³¸ ê²€ìƒ‰ê¸° ê°€ì ¸ì˜¤ê¸°
    retriever = vectorstore.as_retriever(search_kwargs={"k": 7})
    
    # ë‹¨ìˆœ ì²´ì¸ ìƒì„±
    chain = SimpleRAGChain(retriever, vectorstore)
    
    logger.info("RAG ì²´ì¸ ìƒì„± ì™„ë£Œ!")
    return chain



def query_rag_system(chain, query, concert_id=None, concert_info=None):
    """RAG ì‹œìŠ¤í…œì— ì§ˆì˜í•©ë‹ˆë‹¤."""
    logger.info(f"ì§ˆì˜ ì²˜ë¦¬ ì¤‘: '{query}'")
    
    try:
        # 1. ì¿¼ë¦¬ í™•ì¥ (ëŒ€í™” ë§¥ë½ ê³ ë ¤) 
        expanded_query = expand_query(query, chain.conversation_history)
        
        # 2. ê²€ìƒ‰ í•„í„° ì„¤ì • (ë³€ê²½ ì—†ìŒ)
        search_kwargs = {"k": 5}  
        if concert_id:
            search_kwargs["filter"] = {"concert_id": concert_id}
        
        # 3. í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ 
        logger.info(f"í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰: '{expanded_query}'")
        docs = chain.vectorstore.similarity_search(
            expanded_query, 
            **search_kwargs
        )
        logger.info(f"ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
        
        # 4. ì›ë³¸ ì½˜í…ì¸  ì €ì¥ (ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©) (ë³€ê²½ ì—†ìŒ)
        original_contents = {}
        for i, doc in enumerate(docs):
            original_contents[i] = doc.page_content
            
            # ID ë¶€ì—¬ëœ ì½˜í…ì¸ ë¡œ ë³€ê²½
            doc.page_content = f"[ë¬¸ì„œ #{i+1}]\n{doc.page_content}\n[ë¬¸ì„œ #{i+1} ë]"
        
        # 5. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë³€ê²½ ì—†ìŒ)
        context = "\n\n".join([doc.page_content for doc in docs])

        # DB ì •ë³´ ë¬¸ìì—´ ì¤€ë¹„
        db_info = ""
        if concert_info:
            # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ 
            start_times = concert_info.get('start_times', [])
            times_str_list = []
            for time_obj in start_times:
                if hasattr(time_obj, 'strftime'):  # datetime ê°ì²´ì¸ ê²½ìš° ğŸ‘ˆ
                    times_str_list.append(time_obj.strftime('%Y-%m-%d %H:%M:%S'))
                else:  # ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš°
                    times_str_list.append(str(time_obj))
            
            # ì„ ì˜ˆë§¤/ì¼ë°˜ì˜ˆë§¤ ì‹œê°„ ë¬¸ìì—´ ë³€í™˜ 
            adv_res = concert_info.get('advanced_reservation')
            if hasattr(adv_res, 'strftime'):
                adv_res = adv_res.strftime('%Y-%m-%d %H:%M:%S')
                
            res = concert_info.get('reservation')
            if hasattr(res, 'strftime'):
                res = res.strftime('%Y-%m-%d %H:%M:%S')
            
            db_info = f"""
        <DB_ì •ë³´>
        ì½˜ì„œíŠ¸ ì´ë¦„: {concert_info.get('concert_name', 'ì •ë³´ ì—†ìŒ')}
        ê³µì—°ì¥: {concert_info.get('arena_name', 'ì •ë³´ ì—†ìŒ')}
        ì•„í‹°ìŠ¤íŠ¸: {', '.join(concert_info.get('artists', ['ì •ë³´ ì—†ìŒ']))}
        ì„ ì˜ˆë§¤ ì‹œì‘: {adv_res if adv_res else 'ì •ë³´ ì—†ìŒ'}
        ì¼ë°˜ì˜ˆë§¤ ì‹œì‘: {res if res else 'ì •ë³´ ì—†ìŒ'}
        í‹°ì¼“íŒ… í”Œë«í¼: {concert_info.get('ticketing_platform', 'ì •ë³´ ì—†ìŒ')}
        ê³µì—° ì‹œì‘ ì‹œê°„: {', '.join(times_str_list) if times_str_list else 'ì •ë³´ ì—†ìŒ'}
        </DB_ì •ë³´>
        """
        
        # 6. ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ 
        prompt = f"""
ë‹¹ì‹ ì€ ì½˜ì„œíŠ¸ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì¸ 'ì½˜ë¼ë¦¬ë´‡'ì…ë‹ˆë‹¤. 
ì•„ë˜ ì œê³µëœ ì½˜ì„œíŠ¸ ê³µì§€ì‚¬í•­ ì •ë³´ì™€ DB ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë§ëë§ˆë‹¤ 'ë¿Œìš°'ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”. ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”, ë¿Œìš°"

{db_info}

<ì½˜ì„œíŠ¸_ì •ë³´>
{context}
</ì½˜ì„œíŠ¸_ì •ë³´>

ì§ˆë¬¸: {query}

ì§€ì¹¨:
1. ë‹µë³€ ì „ì— ë¨¼ì € ë¬¸ì„œì—ì„œ ì •í™•í•œ ì‚¬ì‹¤ì„ ì°¾ìœ¼ì„¸ìš”. ë¬¸ì„œì— ëª…ì‹œì ìœ¼ë¡œ ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. 
2. ì •í™•í•œ ë‚ ì§œ, ì‹œê°„, ê¸ˆì•¡ ë“± êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ëŠ” ë°˜ë“œì‹œ ë¬¸ì„œì—ì„œ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì„¸ìš”.
3. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ê³µì§€ì‚¬í•­ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ë§í•˜ì„¸ìš”.
4. ê³µì—° ì‹œê°„, ë‚ ì§œì— ê´€í•œ ì§ˆë¬¸ì—ëŠ” ëª¨ë“  ê³µì—° ì¼ìì™€ ì‹œê°„ì„ ë°˜ë“œì‹œ ì „ë¶€ ì•Œë ¤ì£¼ì„¸ìš”. (ì˜ˆ: "5ì›” 17ì¼ì€ ì˜¤í›„ 6ì‹œ, 5ì›” 18ì¼ì€ ì˜¤í›„ 4ì‹œì— ì‹œì‘í•©ë‹ˆë‹¤."). ê·¸ë¦¬ê³  ì…ì¥ì‹œì‘ ì‹œê°„ê³¼ ê³µì—°ì‹œì‘ ì‹œê°„ì€ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ ìœ ì˜í•´ì£¼ì„¸ìš”.
5. ìœ„ì¹˜ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” ì •í™•í•œ ìœ„ì¹˜ ì •ë³´ê°€ ìˆì„ ê²½ìš°ë§Œ ë‹µë³€í•˜ê³ , ì—†ìœ¼ë©´ "ê³µì§€ì‚¬í•­ì—ì„œ ìœ„ì¹˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ë§í•˜ì„¸ìš”.
6. ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ì„ ë•ŒëŠ”, "ê³µì§€ì‚¬í•­ì—ì„œ [íŠ¹ì • ë‚´ìš©]ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ë§í•œ í›„, ê´€ë ¨ëœ ë‹¤ë¥¸ ì •ë³´ê°€ ìˆë‹¤ë©´ í•¨ê»˜ ì œê³µí•˜ì„¸ìš”.
7. ì½˜ì„œíŠ¸ ì´ë¦„, ê³µì—°ì¥, ì•„í‹°ìŠ¤íŠ¸, í‹°ì¼“íŒ… í”Œë«í¼ì´ ë­”ì§€ ë¬»ëŠ” ë‹¨ìˆœí•œ ì§ˆë¬¸ì—ëŠ” DB_ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
8. ë‹µë³€ì˜ ì¶œì²˜ê°€ ë˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì„œ ID í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”. ì§ˆë¬¸ ê´€ë ¨ ì •ë³´ê°€ ì½˜ì„œíŠ¸_ì •ë³´ì— ì „í˜€ ì—†ìœ¼ë©´ "ì—†ìŒ"ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”. DB_ì •ë³´ë¡œë§Œ ë‹µë³€ì„ í•œ ê²½ìš°ì—ë„ "ì—†ìŒ"ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.
9. ì •í™•íˆ ì•„ë˜ í˜•ì‹ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ë‹¤ë¥¸ í˜•ì‹ì´ë‚˜ í‘œí˜„(ì˜ˆ: "ì¶œì²˜:", "ì°¸ê³ :", ë“±)ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

[ë‹µë³€]
ë‹¹ì‹ ì˜ ë‹µë³€ ë‚´ìš©ì„ ì—¬ê¸°ì— ì‘ì„±í•˜ì„¸ìš”.

[ì°¸ì¡°_ë¬¸ì„œ]
#ìˆ«ì ë˜ëŠ” ì—†ìŒ

ì˜ˆì‹œ ì‘ë‹µ 1:
[ë‹µë³€]
ì½˜ì„œíŠ¸ëŠ” 5ì›” 17ì¼ ì˜¤í›„ 6ì‹œì— ì‹œì‘í•©ë‹ˆë‹¤. ë¿Œìš°

[ì°¸ì¡°_ë¬¸ì„œ]
#3

ì˜ˆì‹œ ì‘ë‹µ 2:
[ë‹µë³€]
ì œê³µëœ ì •ë³´ì—ì„œ ë¬¼í’ˆ ë³´ê´€ì†Œì˜ ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ë¬¼í’ˆ ë³´ê´€ì†ŒëŠ” ê³µì—° ì‹œì‘ 6ì‹œê°„ ì „ë¶€í„° ìš´ì˜ë˜ë©°, ì´ìš© ìš”ê¸ˆì€ 5,000ì›ì…ë‹ˆë‹¤. ë¿Œìš°

[ì°¸ì¡°_ë¬¸ì„œ]
#1
"""
        
        system_message = """
ì½˜ë¼ë¦¬ëŠ” ì½˜ì„œíŠ¸ ê´€ëŒì„ ë•ëŠ” ì–´í”Œì´ì´ê³  ë„ˆëŠ” ê·¸ ì–´í”Œ ì†ì—ì„œ ì½˜ì„œíŠ¸ ì •ë³´ë¥¼ ì •í™•íˆ ì•Œë ¤ì£¼ëŠ” ì½˜ë¼ë¦¬ ì±—ë´‡ì´ì•¼. 
ì¤‘ìš”í•œ ê·œì¹™:
1. ì ˆëŒ€ í• ë£¨ì‹œë„¤ì´ì…˜í•˜ì§€ ë§ˆì„¸ìš”! ì œê³µëœ ë¬¸ì„œì— ëª…ì‹œì ìœ¼ë¡œ ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
2. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”. ì‚¬ì‹¤ í™•ì¸ì„ ì² ì €íˆ í•˜ì„¸ìš”.
3. ëª¨ë“  ê³µì—° ë‚ ì§œì™€ ì‹œê°„ì„ ì§ˆë¬¸ ì‹œ í•­ìƒ í•¨ê»˜ ì•Œë ¤ì£¼ì„¸ìš”.
4. ì •í™•í•˜ê²Œ "ë¿Œìš°"ë¡œ ëª¨ë“  ë‹µë³€ì„ ëë‚´ì„¸ìš”.
5. ì ˆëŒ€ë¡œ ê°€ê²©, ì‹œê°„, ìœ„ì¹˜ ë“±ì˜ ì •ë³´ë¥¼ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
6. ì½˜ë¼ë¦¬ ì´ìš©ë²•ì— ëŒ€í•´ ë¬»ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ì˜¨ë³´ë”© í˜ì´ì§€ë¥¼ ì°¸ê³ í•´ë‹¬ë¼ê³  í•˜ì„¸ìš”. 
"""
        
        # 7. GPT í˜¸ì¶œ (ë³€ê²½ ì—†ìŒ)
        from openai import OpenAI
        client = OpenAI()
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
        )
        
        answer_text = response.choices[0].message.content
        logger.info(f"GPT ì‘ë‹µ: {answer_text[:500]}...")
        
        # 8. GPT ì‘ë‹µ íŒŒì‹± (ê°œì„ ) ğŸ‘ˆ
        import re
        answer_match = re.search(r'\[ë‹µë³€\](.*?)(?=\[ì°¸ì¡°_ë¬¸ì„œ\]|\Z)', answer_text, re.DOTALL)
        doc_match = re.search(r'\[ì°¸ì¡°_ë¬¸ì„œ\]\s*(?:#?(\d+)|ì—†ìŒ)', answer_text)
        
        if answer_match:
            answer = answer_match.group(1).strip()
        else:
            answer = answer_text
        
        # 9. ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ (ê°œì„ ) ğŸ‘ˆ
        evidence_coordinates = []
        referenced_doc_id = None
        has_reference = False  # ğŸ‘ˆ ìœ íš¨í•œ ì°¸ì¡° ì—¬ë¶€ ì¶”ì 
        
        if doc_match:
            reference_text = doc_match.group(0).strip()
            if "ì—†ìŒ" in reference_text:
                logger.info("GPT: ê´€ë ¨ ì°¸ì¡° ë¬¸ì„œ ì—†ìŒ")
                # ğŸ‘ˆ "ì—†ìŒ"ì¸ ê²½ìš° ì¢Œí‘œ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ì˜ë„ì ìœ¼ë¡œ ë¹ˆ ë°°ì—´ ìœ ì§€)
            elif doc_match.group(1):
                try:
                    # ì°¸ì¡° ë¬¸ì„œ ID ì¶”ì¶œ (1ë¶€í„° ì‹œì‘)
                    referenced_doc_id = int(doc_match.group(1)) - 1  # 0-based ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                    logger.info(f"ì°¸ì¡°ëœ ë¬¸ì„œ ID: #{referenced_doc_id+1}")
                    has_reference = True  # ğŸ‘ˆ ìœ íš¨í•œ ì°¸ì¡° í‘œì‹œ
                    
                    if 0 <= referenced_doc_id < len(docs):
                        # í•´ë‹¹ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ
                        doc = docs[referenced_doc_id]
                        if hasattr(doc, 'metadata') and 'top_y' in doc.metadata and 'bottom_y' in doc.metadata:
                            evidence_coordinates.append({
                                "top_y": doc.metadata['top_y'],
                                "bottom_y": doc.metadata['bottom_y']
                            })
                            logger.info(f"ë¬¸ì„œ #{referenced_doc_id+1}ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ: top_y={doc.metadata['top_y']}, bottom_y={doc.metadata['bottom_y']}")
                        else:
                            logger.warning(f"ë¬¸ì„œ #{referenced_doc_id+1}ì— ì¢Œí‘œ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                    else:
                        logger.warning(f"ì°¸ì¡°ëœ ë¬¸ì„œ ID #{referenced_doc_id+1}ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ë¬¸ì„œ ìˆ˜: {len(docs)}")
                except (ValueError, IndexError) as e:
                    logger.warning(f"ì°¸ì¡° ë¬¸ì„œ ID ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        else:
            logger.warning("GPT ì‘ë‹µì—ì„œ ì°¸ì¡° ë¬¸ì„œ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # 10. ë¬¸ì„œ ì½˜í…ì¸  ë³µì› (ë³€ê²½ ì—†ìŒ)
        for i, doc in enumerate(docs):
            if i in original_contents:
                doc.page_content = original_contents[i]
        
        # 11. ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸ (ì‹ ê·œ ì¶”ê°€) ğŸ‘ˆ
        chain.conversation_history.append({
            "user": query,
            "bot": answer
        })
        
        # ëŒ€í™” ê¸°ë¡ í¬ê¸° ì œí•œ (ìµœëŒ€ 5ê°œ í„´ë§Œ ìœ ì§€) ğŸ‘ˆ
        if len(chain.conversation_history) > 5:
            chain.conversation_history = chain.conversation_history[-5:]
        
        # 12. ì‘ë‹µ êµ¬ì„± (ê°œì„ ) ğŸ‘ˆ
        response = {
            "answer": answer,
            "source_documents": [],
            "evidence_coordinates": evidence_coordinates if has_reference else [],  # ğŸ‘ˆ ì°¸ì¡°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ì¢Œí‘œ ë°°ì—´
            "referenced_doc_id": referenced_doc_id
        }
        
        # ì†ŒìŠ¤ ë¬¸ì„œ ì¶”ê°€ (ì›ë³¸ ì½˜í…ì¸ ë¡œ ë³µì›)
        response["source_documents"] = [
            {
                "content": doc.page_content,
                "metadata": doc.metadata if hasattr(doc, 'metadata') else {}
            } for doc in docs
        ]
        
        logger.info("ì§ˆì˜ ì²˜ë¦¬ ì™„ë£Œ")
        return response
        
    except Exception as e:
        logger.error(f"ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # ê°„ì†Œí™”ëœ ë‹µë³€ ë°˜í™˜
        return {
            "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤, ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ë¿Œìš°...",
            "error": str(e),
            "source_documents": [],
            "evidence_coordinates": []
        }