import os
import logging
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

def split_documents(paragraphs, concert_info):
    """ë¬¸ë‹¨ë“¤ì„ ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    logger.info("ë¬¸ë‹¨ì„ ì²­í¬ë¡œ ë¶„í•  ì¤‘...")
    
    # RecursiveCharacterTextSplitter ì„¤ì •
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,  # ì²­í¬ í¬ê¸°
        chunk_overlap=50,  # ì²­í¬ ê°„ ì¤‘ë³µ (ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´)
        separators=["\n\n", "\n", ".", ",", " ", ""]
    )
    
    all_texts = []
    all_metadatas = []
    
    # ê° ë¬¸ë‹¨ì„ ì²˜ë¦¬
    for i, paragraph in enumerate(paragraphs):
        text = paragraph["text"]

        para_metadata = paragraph["metadata"]  # ê·¸ë£¹í™”ëœ ë¬¸ë‹¨ì—ëŠ” í•­ìƒ metadataê°€ ìˆìŒ

        metadata = {
            "concert_id": concert_info["concert_id"],
            "paragraph_id": i,
            "top_y": para_metadata.get("top_y"),
            "bottom_y": para_metadata.get("bottom_y"),
            "category": para_metadata.get("category", "ì¼ë°˜ ì •ë³´")
        }

        
        # ì½˜ì„œíŠ¸ ì •ë³´ ë©”íƒ€ë°ì´í„° ì¶”ê°€ 
        metadata["concert_name"] = concert_info["concert_name"]
        metadata["arena_name"] = concert_info["arena_name"]
        
        # ì•„í‹°ìŠ¤íŠ¸ ì •ë³´ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€ (ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ) 
        artists = concert_info.get("artist_name", [])
        if artists:
            metadata["artists"] = ", ".join(artists)
        
        # í‹°ì¼“íŒ… ì •ë³´ ì¶”ê°€ 
        metadata["ticketing_platform"] = concert_info.get("ticketing_platform")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        splits = text_splitter.split_text(text)
        
        # ê° ë¶„í• ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for j, split in enumerate(splits):
            all_texts.append(split)
            split_metadata = metadata.copy()
            split_metadata["chunk_id"] = f"{i}-{j}"
            split_metadata["text_snippet"] = split[:100]  # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë¯¸ë¦¬ë³´ê¸°
            all_metadatas.append(split_metadata)
    
    logger.info(f"ì´ {len(all_texts)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return all_texts, all_metadatas

# def create_rag_chain(vectorstore):
    """RAG ì§ˆì˜ì‘ë‹µ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    logger.info("RAG ì²´ì¸ ìƒì„± ì¤‘...")
    
    # OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # LLM ì„¤ì •
    llm = ChatOpenAI(
        api_key=openai_api_key,
        model_name="gpt-3.5-turbo",  # ë˜ëŠ” "gpt-4o"
        temperature=0
    )
    
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìˆ˜ì •
    prompt_template = """ 
ë‹¹ì‹ ì€ ì½˜ì„œíŠ¸ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì¸ 'ì½˜ë¼ë¦¬ë´‡'ì…ë‹ˆë‹¤. 
ì•„ë˜ ì œê³µëœ ì½˜ì„œíŠ¸ ê³µì§€ì‚¬í•­ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë§ëë§ˆë‹¤ 'ë¿Œìš°'ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”. ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”, ë¿Œìš°"

<ì½˜ì„œíŠ¸_ì •ë³´>
{context}
</ì½˜ì„œíŠ¸_ì •ë³´>

ì§ˆë¬¸: {question}

ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ì½˜ì„œíŠ¸ ê³µì§€ì‚¬í•­ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
2. ë‹µë³€ì˜ ì¶œì²˜ê°€ ë˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì„œ IDë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
3. ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì‘ë‹µí•˜ì„¸ìš”:

[ë‹µë³€]
ë‹¹ì‹ ì˜ ë‹µë³€ ë‚´ìš©ì„ ì—¬ê¸°ì— ì‘ì„±í•˜ì„¸ìš”.

[ì°¸ì¡°_ë¬¸ì„œ]
ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ID í•˜ë‚˜ë§Œ ì„ íƒ (ì˜ˆ: #2)

ì˜ˆì‹œ:
[ë‹µë³€]
ì½˜ì„œíŠ¸ëŠ” 5ì›” 17ì¼ ì˜¤í›„ 6ì‹œì— ì‹œì‘í•©ë‹ˆë‹¤. ë¿Œìš°

[ì°¸ì¡°_ë¬¸ì„œ]
#3
"""
    PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
    
        
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    document_prompt = PromptTemplate(
        input_variables=["page_content", "id"], 
        template="""[ë¬¸ì„œ #{id}]
{page_content}
[ë¬¸ì„œ #{id} ë]"""
    )
    
    def format_docs_with_ids(docs):
        doc_strings = []
        for i, doc in enumerate(docs):
            # ê° ë¬¸ì„œì— ID ë¶€ì—¬
            doc_strings.append(document_prompt.format(
                page_content=doc.page_content,
                id=i+1
            ))
        return "\n\n".join(doc_strings)

    chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={
            "prompt": PROMPT,
            # "document_prompt": PromptTemplate(
            #     input_variables=["page_content"], 
            #     template="{page_content}"
            # ),
            "document_variable_name": "context",
            "document_separator": "\n\n",
            "document_prompt_template": "{page_content}", 
            "format_documents_function": format_docs_with_ids 
        }
    )
    
    logger.info("RAG ì²´ì¸ ìƒì„± ì™„ë£Œ!")
    return chain

# ê·¸ëƒ¥ ì²«ë²ˆì§¸ ë¬¸ì„œ ì¢Œí‘œ ì‚¬ìš© ver
# def create_rag_chain(vectorstore):
    """RAG ì§ˆì˜ì‘ë‹µ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    logger.info("RAG ì²´ì¸ ìƒì„± ì¤‘...")
    
    # OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # LLM ì„¤ì •
    llm = ChatOpenAI(
        api_key=openai_api_key,
        model_name="gpt-3.5-turbo",  # ë˜ëŠ” "gpt-4o"
        temperature=0
    )
    
    # ê¸°ë³¸ ê²€ìƒ‰ê¸° ê°€ì ¸ì˜¤ê¸°
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìˆ˜ì • - ë¬¸ì„œ IDê°€ ì—†ëŠ” ê¸°ë³¸ ë²„ì „
    prompt_template = """
ë‹¹ì‹ ì€ ì½˜ì„œíŠ¸ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì¸ 'ì½˜ë¼ë¦¬ë´‡'ì…ë‹ˆë‹¤. 
ì•„ë˜ ì œê³µëœ ì½˜ì„œíŠ¸ ê³µì§€ì‚¬í•­ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë§ëë§ˆë‹¤ 'ë¿Œìš°'ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”. ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”, ë¿Œìš°"

<ì½˜ì„œíŠ¸_ì •ë³´>
{context}
</ì½˜ì„œíŠ¸_ì •ë³´>

ì§ˆë¬¸: {question}

ë‹µë³€ì„ í•  ë•ŒëŠ” ê³µì§€ì‚¬í•­ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
    PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
    
    # ê¸°ë³¸ ë¬¸ì„œ í”„ë¡¬í”„íŠ¸
    document_prompt = PromptTemplate(
        input_variables=["page_content"], 
        template="{page_content}"
    )
    
    # ê¸°ë³¸ ì²´ì¸ ìƒì„±
    chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={
            "prompt": PROMPT,
            "document_prompt": document_prompt
        }
    )
    
    logger.info("RAG ì²´ì¸ ìƒì„± ì™„ë£Œ!")
    return chain

# def query_rag_system(chain, query, concert_id=None):
    """RAG ì‹œìŠ¤í…œì— ì§ˆì˜í•©ë‹ˆë‹¤."""
    logger.info(f"ì§ˆì˜ ì²˜ë¦¬ ì¤‘: '{query}'")
    
    # ê²€ìƒ‰ í•„í„° ì„¤ì • (íŠ¹ì • ì½˜ì„œíŠ¸ë§Œ ê²€ìƒ‰)
    search_kwargs = {"k": 5}
    if concert_id:
        search_kwargs["filter"] = {"concert_id": concert_id}
        # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        if hasattr(chain, 'retriever'):
            retriever = chain.retriever
            retriever.search_kwargs.update(search_kwargs)
    
    try:
        result = chain.invoke({"query": query})
        
        # ê²°ê³¼ í˜•ì‹ í™•ì¸ ë° ì²˜ë¦¬
        if isinstance(result, dict) and "result" in result:
            answer_text = result.get("result", "ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            logger.info(f"GPT ì‘ë‹µ: {result['result'][:200]}...")

        elif isinstance(result, str):
            answer_text = result
        else:
            logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ í˜•ì‹: {type(result)}")
            answer_text = str(result)

        import re
        # answer_match = re.search(r'\[ë‹µë³€\](.*?)(?=\[ì¦ê±°_ì¢Œí‘œ\]|\Z)', answer_text, re.DOTALL) 
        # coords_match = re.search(r'\[ì¦ê±°_ì¢Œí‘œ\](.*?)(?=\[|\Z)', answer_text, re.DOTALL)

        answer_match = re.search(r'\[ë‹µë³€\](.*?)(?=\[ì°¸ì¡°_ë¬¸ì„œ\]|\Z)', answer_text, re.DOTALL)
        doc_match = re.search(r'\[ì°¸ì¡°_ë¬¸ì„œ\]\s*#?(\d+)', answer_text)

        if answer_match: 
            answer = answer_match.group(1).strip() 
        else: 
            answer = answer_text
        
        # ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ ìˆ˜ì •
        evidence_coordinates = []
        # if coords_match:
        #     coords_text = coords_match.group(1).strip()
            
        #     # "ì—†ìŒ" ì¼€ì´ìŠ¤ í™•ì¸
        #     if "ì—†ìŒ" in coords_text or "none" in coords_text.lower():
        #         # ì¦ê±° ìë£Œ ì—†ìŒ - ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìœ ì§€
        #         logger.info("GPTê°€ 'ì—†ìŒ'ì´ë¼ê³  ì‘ë‹µí–ˆìŠµë‹ˆë‹¤")
        #     else:
        #         # ì¢Œí‘œ íŒŒì‹± ì‹œë„ (ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ í‚¤-ê°’ ìŒ í˜•ì‹)
        #         # ì •ê·œì‹ íŒ¨í„´ ê°œì„ 
        #         coords_pattern = r'top_y\s*=\s*(\d+)\s*,\s*bottom_y\s*=\s*(\d+)'
        #         coords_values = re.search(coords_pattern, coords_text)
                
        #         if coords_values:
        #             top_y = int(coords_values.group(1))
        #             bottom_y = int(coords_values.group(2))
        #             logger.info(f"íŒŒì‹±ëœ ì¢Œí‘œ: top_y={top_y}, bottom_y={bottom_y}")
                    
        #             evidence_coordinates.append({
        #                 "top_y": top_y,
        #                 "bottom_y": bottom_y
        #             })
        #         else:
        #             logger.warning(f"ì¢Œí‘œ í˜•ì‹ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤: '{coords_text}'")
        # í†µì¼ëœ ê²°ê³¼ í˜•ì‹

        referenced_doc_id = None
        if doc_match:
            try:
                # ì°¸ì¡° ë¬¸ì„œ ID ì¶”ì¶œ (1ë¶€í„° ì‹œì‘)
                referenced_doc_id = int(doc_match.group(1)) - 1  # 0-based ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                logger.info(f"ì°¸ì¡°ëœ ë¬¸ì„œ ID: #{referenced_doc_id+1}")
                
                if 0 <= referenced_doc_id < len(result["source_documents"]):
                    # í•´ë‹¹ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ
                    doc = result["source_documents"][referenced_doc_id]
                    if hasattr(doc, 'metadata') and 'top_y' in doc.metadata and 'bottom_y' in doc.metadata:
                        evidence_coordinates.append({
                            "top_y": doc.metadata['top_y'],
                            "bottom_y": doc.metadata['bottom_y']
                        })
                        logger.info(f"ë¬¸ì„œ #{referenced_doc_id+1}ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ: top_y={doc.metadata['top_y']}, bottom_y={doc.metadata['bottom_y']}")
                    else:
                        logger.warning(f"ë¬¸ì„œ #{referenced_doc_id+1}ì— ì¢Œí‘œ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                else:
                    logger.warning(f"ì°¸ì¡°ëœ ë¬¸ì„œ ID #{referenced_doc_id+1}ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ë¬¸ì„œ ìˆ˜: {len(result['source_documents'])}")
            except (ValueError, IndexError) as e:
                logger.warning(f"ì°¸ì¡° ë¬¸ì„œ ID ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        else:
            logger.warning("GPT ì‘ë‹µì—ì„œ ì°¸ì¡° ë¬¸ì„œ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # IDë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì¢Œí‘œ ì‚¬ìš© (í´ë°±) ğŸ‘ˆ
            if "source_documents" in result and len(result["source_documents"]) > 0:
                doc = result["source_documents"][0]
                if hasattr(doc, 'metadata') and 'top_y' in doc.metadata and 'bottom_y' in doc.metadata:
                    evidence_coordinates.append({
                        "top_y": doc.metadata['top_y'],
                        "bottom_y": doc.metadata['bottom_y']
                    })
                    logger.info(f"ID ì—†ì–´ ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì¢Œí‘œ ì‚¬ìš©: top_y={doc.metadata['top_y']}, bottom_y={doc.metadata['bottom_y']}")

        response = {
            "answer": answer,
            "source_documents": [],
            "evidence_coordinates": evidence_coordinates,
            "referenced_doc_id": referenced_doc_id
        }
        
        # ì†ŒìŠ¤ ë¬¸ì„œ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        if isinstance(result, dict) and "source_documents" in result:
            response["source_documents"] = [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata if hasattr(doc, 'metadata') else {}
                } for doc in result["source_documents"]
            ]
        #     response["evidence_coordinates"] = [
        #     {
        #         "top_y": doc.metadata["top_y"],
        #         "bottom_y": doc.metadata["bottom_y"],
        #         "category": doc.metadata.get("category", "ì¼ë°˜ ì •ë³´")
        #     } for doc in result["source_documents"] 
        #     if "top_y" in doc.metadata and "bottom_y" in doc.metadata
        # ]
        
        logger.info("ì§ˆì˜ ì²˜ë¦¬ ì™„ë£Œ")
        return response
    
    except Exception as e:
        logger.error(f"ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(traceback.format_exc()) 
        # ê°„ì†Œí™”ëœ ë‹µë³€ ë°˜í™˜
        return {
            "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤, ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ë¿Œìš°...",
            "error": str(e),
            "source_documents": [],
            "evidence_coordinates": []
        }

# gpt ë‹¤ì‹œ ì‹œë„ ver
def create_rag_chain(vectorstore):
    """RAG ì§ˆì˜ì‘ë‹µ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    logger.info("RAG ì²´ì¸ ìƒì„± ì¤‘...")
    
    # ë‹¨ìˆœ ë˜í¼ ê°ì²´ ìƒì„± (ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ê¸°ë§Œ í¬í•¨)
    class SimpleRAGChain:
        def __init__(self, retriever):
            self.retriever = retriever
            
        def __call__(self, query_dict):
            # ë”ë¯¸ ë©”ì„œë“œ (ì‹¤ì œë¡œëŠ” query_rag_systemì—ì„œ ì²˜ë¦¬)
            return {"result": "This is a placeholder", "source_documents": []}
    
    # ê¸°ë³¸ ê²€ìƒ‰ê¸° ê°€ì ¸ì˜¤ê¸°
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    
    # ë‹¨ìˆœ ì²´ì¸ ìƒì„±
    chain = SimpleRAGChain(retriever)
    
    logger.info("RAG ì²´ì¸ ìƒì„± ì™„ë£Œ!")
    return chain


# ê·¸ëƒ¥ ì²«ë²ˆì§¸ ë¬¸ì„œ ì¢Œí‘œ ì‚¬ìš© ver
# def query_rag_system(chain, query, concert_id=None):
#     """RAG ì‹œìŠ¤í…œì— ì§ˆì˜í•©ë‹ˆë‹¤."""
#     logger.info(f"ì§ˆì˜ ì²˜ë¦¬ ì¤‘: '{query}'")
    
#     # ê²€ìƒ‰ í•„í„° ì„¤ì • (íŠ¹ì • ì½˜ì„œíŠ¸ë§Œ ê²€ìƒ‰)
#     if concert_id and hasattr(chain, 'retriever'):
#         # ê²€ìƒ‰ í•„í„° ì„¤ì • ì‹œë„
#         try:
#             chain.retriever.search_kwargs.update({"filter": {"concert_id": concert_id}})
#         except Exception as e:
#             logger.warning(f"ê²€ìƒ‰ í•„í„° ì„¤ì • ì‹¤íŒ¨ (ë¬´ì‹œë¨): {str(e)}")
    
#     try:
#         # ì›ë³¸ ì¿¼ë¦¬ë¡œ ì²´ì¸ ì‹¤í–‰
#         result = chain.invoke({"query": query})
        
#         # ê²°ê³¼ í˜•ì‹ í™•ì¸ ë° ì²˜ë¦¬
#         if isinstance(result, dict) and "result" in result:
#             answer_text = result.get("result", "ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#             logger.info(f"GPT ì‘ë‹µ: {result['result'][:200]}...")
#         elif isinstance(result, str):
#             answer_text = result
#         else:
#             logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ í˜•ì‹: {type(result)}")
#             answer_text = str(result)
        
#         # ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ
#         evidence_coordinates = []
        
#         # ì†ŒìŠ¤ ë¬¸ì„œì—ì„œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ì„ íƒ
#         if "source_documents" in result and result["source_documents"]:
#             # ê¸°ì¡´ ì‹œìŠ¤í…œì—ì„œëŠ” ë¬¸ì„œ IDê°€ ì—†ìœ¼ë¯€ë¡œ ì²« ë²ˆì§¸ ë¬¸ì„œ ì‚¬ìš©
#             best_doc = result["source_documents"][0]
            
#             # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ
#             if hasattr(best_doc, 'metadata') and 'top_y' in best_doc.metadata and 'bottom_y' in best_doc.metadata:
#                 evidence_coordinates.append({
#                     "top_y": best_doc.metadata['top_y'],
#                     "bottom_y": best_doc.metadata['bottom_y']
#                 })
#                 logger.info(f"ë¬¸ì„œ ì¢Œí‘œ ì¶”ì¶œ: top_y={best_doc.metadata['top_y']}, bottom_y={best_doc.metadata['bottom_y']}")
        
#         # ì‘ë‹µ êµ¬ì„±
#         response = {
#             "answer": answer_text,
#             "source_documents": [],
#             "evidence_coordinates": evidence_coordinates
#         }
        
#         # ì†ŒìŠ¤ ë¬¸ì„œ ì¶”ê°€
#         if "source_documents" in result:
#             response["source_documents"] = [
#                 {
#                     "content": doc.page_content,
#                     "metadata": doc.metadata if hasattr(doc, 'metadata') else {}
#                 } for doc in result["source_documents"]
#             ]
        
#         logger.info("ì§ˆì˜ ì²˜ë¦¬ ì™„ë£Œ")
#         return response
        
#     except Exception as e:
#         logger.error(f"ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
#         import traceback
#         logger.error(traceback.format_exc())
        
#         # ê°„ì†Œí™”ëœ ë‹µë³€ ë°˜í™˜
#         return {
#             "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤, ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ë¿Œìš°...",
#             "error": str(e),
#             "source_documents": [],
#             "evidence_coordinates": []
#         }
    


# gpt ë‹¤ì‹œ ì‹œë„ ver
def query_rag_system(chain, query, concert_id=None):
    """RAG ì‹œìŠ¤í…œì— ì§ˆì˜í•©ë‹ˆë‹¤."""
    logger.info(f"ì§ˆì˜ ì²˜ë¦¬ ì¤‘: '{query}'")
    
    # ê²€ìƒ‰ í•„í„° ì„¤ì • (íŠ¹ì • ì½˜ì„œíŠ¸ë§Œ ê²€ìƒ‰)
    if concert_id and hasattr(chain, 'retriever'):
        # ê²€ìƒ‰ í•„í„° ì„¤ì • ì‹œë„
        try:
            chain.retriever.search_kwargs.update({"filter": {"concert_id": concert_id}})
        except Exception as e:
            logger.warning(f"ê²€ìƒ‰ í•„í„° ì„¤ì • ì‹¤íŒ¨ (ë¬´ì‹œë¨): {str(e)}")
    
    try:
        # 1. ìˆ˜ë™ìœ¼ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
        docs = None
        try:
            # ì§ì ‘ ê²€ìƒ‰
            docs = chain.retriever.get_relevant_documents(query)
            logger.info(f"ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ë°±ì—… ë°©ì‹: ì²´ì¸ í˜¸ì¶œ í›„ source_documents ì‚¬ìš©
            result = chain({"query": query})
            if "source_documents" in result:
                docs = result["source_documents"]
                logger.info(f"ë°±ì—… ë°©ì‹ìœ¼ë¡œ {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
        
        if not docs:
            raise ValueError("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # 2. ì›ë³¸ ì½˜í…ì¸  ì €ì¥
        original_contents = {}
        for i, doc in enumerate(docs):
            original_contents[i] = doc.page_content
            
            # ID ë¶€ì—¬ëœ ì½˜í…ì¸ ë¡œ ë³€ê²½
            doc.page_content = f"[ë¬¸ì„œ #{i+1}]\n{doc.page_content}\n[ë¬¸ì„œ #{i+1} ë]"
        
        # 3. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # 4. ì§ì ‘ OpenAI API í˜¸ì¶œ (LangChainì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ)
        from openai import OpenAI
        client = OpenAI()
        
        # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
        prompt = f"""
ë‹¹ì‹ ì€ ì½˜ì„œíŠ¸ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì¸ 'ì½˜ë¼ë¦¬ë´‡'ì…ë‹ˆë‹¤. 
ì•„ë˜ ì œê³µëœ ì½˜ì„œíŠ¸ ê³µì§€ì‚¬í•­ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë§ëë§ˆë‹¤ 'ë¿Œìš°'ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”. ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”, ë¿Œìš°"

<ì½˜ì„œíŠ¸_ì •ë³´>
{context}
</ì½˜ì„œíŠ¸_ì •ë³´>

ì§ˆë¬¸: {query}

ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ì½˜ì„œíŠ¸ ê³µì§€ì‚¬í•­ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
2. ë‹µë³€ì˜ ì¶œì²˜ê°€ ë˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì„œ IDë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
3. ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì‘ë‹µí•˜ì„¸ìš”:

[ë‹µë³€]
ë‹¹ì‹ ì˜ ë‹µë³€ ë‚´ìš©ì„ ì—¬ê¸°ì— ì‘ì„±í•˜ì„¸ìš”.

[ì°¸ì¡°_ë¬¸ì„œ]
ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ID í•˜ë‚˜ë§Œ ì„ íƒ (ì˜ˆ: #2)
"""
        
        # API í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì½˜ì„œíŠ¸ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ëŠ” ì½˜ë¼ë¦¬ ì±—ë´‡ì´ì•¼."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
        )
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        answer_text = response.choices[0].message.content
        logger.info(f"GPT ì‘ë‹µ: {answer_text[:200]}...")
        
        # 5. GPT ì‘ë‹µ íŒŒì‹±
        import re
        answer_match = re.search(r'\[ë‹µë³€\](.*?)(?=\[ì°¸ì¡°_ë¬¸ì„œ\]|\Z)', answer_text, re.DOTALL)
        doc_match = re.search(r'\[ì°¸ì¡°_ë¬¸ì„œ\]\s*#?(\d+)', answer_text)
        
        if answer_match:
            answer = answer_match.group(1).strip()
        else:
            answer = answer_text
        
        # 6. ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ
        evidence_coordinates = []
        referenced_doc_id = None
        
        if doc_match:
            try:
                # ì°¸ì¡° ë¬¸ì„œ ID ì¶”ì¶œ (1ë¶€í„° ì‹œì‘)
                referenced_doc_id = int(doc_match.group(1)) - 1  # 0-based ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                logger.info(f"ì°¸ì¡°ëœ ë¬¸ì„œ ID: #{referenced_doc_id+1}")
                
                if 0 <= referenced_doc_id < len(docs):
                    # í•´ë‹¹ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ
                    doc = docs[referenced_doc_id]
                    if hasattr(doc, 'metadata') and 'top_y' in doc.metadata and 'bottom_y' in doc.metadata:
                        evidence_coordinates.append({
                            "top_y": doc.metadata['top_y'],
                            "bottom_y": doc.metadata['bottom_y']
                        })
                        logger.info(f"ë¬¸ì„œ #{referenced_doc_id+1}ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ: top_y={doc.metadata['top_y']}, bottom_y={doc.metadata['bottom_y']}")
                    else:
                        logger.warning(f"ë¬¸ì„œ #{referenced_doc_id+1}ì— ì¢Œí‘œ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                else:
                    logger.warning(f"ì°¸ì¡°ëœ ë¬¸ì„œ ID #{referenced_doc_id+1}ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ë¬¸ì„œ ìˆ˜: {len(docs)}")
            except (ValueError, IndexError) as e:
                logger.warning(f"ì°¸ì¡° ë¬¸ì„œ ID ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        else:
            logger.warning("GPT ì‘ë‹µì—ì„œ ì°¸ì¡° ë¬¸ì„œ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # IDë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì¢Œí‘œ ì‚¬ìš© (í´ë°±)
            if docs:
                doc = docs[0]
                if hasattr(doc, 'metadata') and 'top_y' in doc.metadata and 'bottom_y' in doc.metadata:
                    evidence_coordinates.append({
                        "top_y": doc.metadata['top_y'],
                        "bottom_y": doc.metadata['bottom_y']
                    })
                    logger.info(f"ID ì—†ì–´ ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì¢Œí‘œ ì‚¬ìš©: top_y={doc.metadata['top_y']}, bottom_y={doc.metadata['bottom_y']}")
        
        # 7. ë¬¸ì„œ ì½˜í…ì¸  ë³µì›
        for i, doc in enumerate(docs):
            if i in original_contents:
                doc.page_content = original_contents[i]
        
        # 8. ì‘ë‹µ êµ¬ì„±
        response_dict = {
            "answer": answer,
            "source_documents": [],
            "evidence_coordinates": evidence_coordinates,
            "referenced_doc_id": referenced_doc_id
        }
        
        # ì†ŒìŠ¤ ë¬¸ì„œ ì¶”ê°€ (ì›ë³¸ ì½˜í…ì¸ ë¡œ ë³µì›)
        response_dict["source_documents"] = [
            {
                "content": doc.page_content,
                "metadata": doc.metadata if hasattr(doc, 'metadata') else {}
            } for doc in docs
        ]
        
        logger.info("ì§ˆì˜ ì²˜ë¦¬ ì™„ë£Œ")
        return response_dict
        
    except Exception as e:
        logger.error(f"ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # ê°„ì†Œí™”ëœ ë‹µë³€ ë°˜í™˜
        return {
            "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤, ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ë¿Œìš°...",
            "error": str(e),
            "source_documents": [],
            "evidence_coordinates": []
        }